\documentclass[12pt]{book}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{cite}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

%\textwidth = 6.5 in
%\textheight = 9 in
%\oddsidemargin = 0.0 in
%\evensidemargin = 0.0 in
%\topmargin = 0.0 in
%\headheight = 0.0 in
%\headsep = 0.0 in
%\parskip = 0.2in
%\parindent = 0.0in
%\bibliographystyle{unsrt}
\bibliographystyle{tlcpub2}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

\title{\textsc{NISE Manual\\ Version 3.4}}
\author{Thomas la Cour Jansen\\ University of Groningen}
%\date{\today\\ \includegraphics{2ds.pdf}}
\date{\today\\ \includegraphics{cover.png}}
\begin{document}
\maketitle

\thispagestyle{empty}
\rule{0mm}{0mm}
%\vfill
\noindent
\thispagestyle{empty}
Front cover: 
%\\[4ex]
%Thomas la Cour Jansen,\\[2ex]
\copyright \/ T.l.C. Jansen, 2017
\setcounter{page}{1}
\thispagestyle{empty}
\clearpage

\addcontentsline{toc}{chapter}{Contents}
\setcounter{page}{1}
\include{tableofcontents}
%\addtocontents{toc}{\protect\thispagestyle{empty}}
%\tableofcontents

\chapter{Introduction}
The main developer of the NISE3 code is Thomas la Cour Jansen. Please, cite the appropriate
references \cite{Jansen.2006.JPCB.110.22910,Jansen.2009.ACR.42.1405,Jansen.2010.JCP.132.224503,Liang.2012.JCTC.8.1706,Liang.2013.JPCL.4.448}
when publishing work using this code. The code allows the calculation of the linear absorption,linear dichroism, sum-frequency generation, two-dimensional spectra (IR,UVvis, and SFG), population transfer, exciton diffusion and integrated anisotropy using the full nonadiabatic semi-classical numerical integration of the Schr\"{o}dinger equation approach
\cite{Jansen.2009.ACR.42.1405} and the sparse matrix optimization approach \cite{Liang.2012.JCTC.8.1706}. The code allows treating spectra of diverse systems involving
intra- and intermolecular energy transfer\cite{Jansen.2006.JPCB.110.22910,Cringus.2007.JCP.127.084507,Jansen.2008.BJ.94.1818,Dijkstra.2010.JPCA.114.7315,Jansen.2010.JCP.132.224503}, non-Gaussian dynamics \cite{Jansen.2009.JPCA.113.6260,Roy.2011.JPCB.115.5431}, surfaces \cite{Liang.2013.JPCL.4.448}, and chemical exchange \cite{Jansen.2007.JCP.127.234502}. This manual is not intended as an introduction to two-dimensional
spectroscopy. The user is directed to the references including recent reviews \cite{Hamm.1998.JPCB.102.6123,Hochstrasser.2001.CP.266.273,Cho.2008.CR.108.1331,Mukamel.2000.ARPC.51.691,Jansen.2009.ACR.42.1405} and books
for more information \cite{Cho.2009.B01,Mukamel.1995.B01,Hamm.2011.B01}.

The code is made available as open source on github.com as link {\tt GHlacour/NISE\_2017}. The version available is intended to be friendly for developers to contribute improvements. Changes in the code are at own risk and should be reported clearly in publications. Developers wanting to contribute to the official version of the code are suggested to contact the main developer in advance to ensure that the contribution is not conflicting with that of other developers. More information for developers is given in Chapter \ref{chap:developers}. Feedback on the program and the manual are welcome via e-mail: t.l.c.jansen@rug.nl. 

Current developers include: Thomas la Cour Jansen (main developer), Floris Westerman (cmake and MPI implementation), and members of the Computational Spectroscopy group at the University of Groningen.

The code use wavenumbers for frequencies and times are femtoseconds. The transition
dipoles and transition polarizabilities may be given in any desired units.

\chapter{Installation and options}
NISE has a number of dependencies:
\begin{itemize}
\item FFTW3 library, possibly with OpenMP/MPI support. See \url{http://www.fftw.org/}.
\item LAPACK library, often preinstalled. See \url{http://www.netlib.org/lapack/}.
\item CMake v3.10 or higher
\item MPI v3 implementation, such as OpenMPI, MPICH (Unix) or MS-MPI (Windows)
\item Modern C compiler, implementing a recent OpenMP version
\item LaTeX + BibTex distribution if you want to build the documentation. Not required
\item DISLIN, python, MATLAB, or gnuplot, for plotting the results using the included scripts
\end{itemize}
As some FFTW3 installations do not come with the correct CMake compatibility, it might be useful to use \href{https://github.com/microsoft/vcpkg}{vcpkg} as package manager for C++ libraries. This package manager is cross-platform compatible.

\section{Building}
In order to build and compile the NISE software, it is recommended to use the CMake build system. This will try to automatically link the libraries and pick the correct compiler settings.
\begin{enumerate}
\item Extract the source code files
\item Create a \texttt{build} directory in the main folder
\item Run \texttt{cmake ..} inside this new {\tt build} directory.
\item If {\tt cmake} was successful, run {\tt make} in the same directory to start compilation.
\item All executables should be available in a new {\tt bin} directory in the main folder.
\end{enumerate}

There are several options you can provide to the {\tt cmake} command in order to customize your build:
\begin{itemize}
\item {\tt -DCMAKE\_BUILD\_TYPE}: By default, this is set to {\tt RelWithDebInfo}. Other options include {\tt Debug}, {\tt Release} and {\tt MinSizeRel}. Refer to the CMake documentation for more information
\item {\tt -DGENERATE\_DOCS}: When not set, CMake will attempt to compile this documentation only when building a Release build. You can override this by setting this variable to {\tt true} or {\tt false}.
\item {\tt -DACCURATE\_MATHS}: When set, CMake will compile the code to use accurate mathematics implementations (as is default when using a C compiler). When not set, the compiler will use the fast-math option ({\tt -ffast-math}, {\tt /fp:fast} or equivalent) which may yield upto 2x speed-up at the minor cost of numerical accuracy.
\end{itemize}

After running CMake, the following build targets will have been provided for {\tt make}:
\begin{itemize}
\item {\tt all}: Same as not providing a build target, will build all source code for the program, but will skip the documentation and the examples
\item {\tt 2DFFT}: Will build the 2DFFT executable, used to process results
\item {\tt translate}: Will build the translation utility, used to convert between input formats
\item {\tt NISE}: Will build the main NISE executable
\item {\tt doc}: Will build this documentation from scratch
\item {\tt examples}: Will build the code necessary for the examples, used later in this document.
\end{itemize}

\section{Installation Trouble Shooting}
If the automatic installation procedure outlined above does not work this section contains a few potential solutions. It is of course important first to verify that the libraries specified in the dependencies are available.

If the FFTW libraries are not detected by the cmake routine the follwing cmake options may be specified by hand:
\begin{verbatim}
cmake .. -DFFTW_ROOT=/cm/shared/apps/fftw/openmpi/gcc/64/3.3.8/lib
 -DFFTW_LIBRARY=/cm/shared/apps/fftw/openmpi/gcc/64/3.3.8/lib
 -DFFTW_INCLUDE_DIRS=/cm/shared/apps/fftw/openmpi/gcc/64/3.3.8/include
\end{verbatim}
The names of the directory locations nust then be changed to the actual locations on the given system.  The example abouve is for a fftw library compiled with the gcc compiler.

The program can also be installed on the Mac OSX system. However, the standard compiler does not come with OpenMP support. An OpenMP library must therefore be installed first. This can be done using the homebrew system. Details of the installation of homebrew are given on \texttt{http://brew.sh}. Then an OpenMP library as lipomp can be installed with \texttt{brew install libomp}. If the version of cmake is also not recent enough a suitable cmake version can be installed with homebrew as well. Finally, the programme can be build following the general instructions above for building, but with useing the command:\\
\begin{verbatim}
/usr/local/bin/cmake .. -DCMAKE_C_COMPILER="clang"
 -DOpenMP_C_LIB_NAMES="libomp" -DOpenMP_CXX_LIB_NAMES="libomp"
 -DOpenMP_libomp_LIBRARY="/usr/local/lib/libomp.dylib"
 -DOpenMP_C_FLAGS="-Xpreprocessor -fopenmp /usr/local/lib/libomp.dylib
 -I/usr/local/opt/include"
\end{verbatim}
Here, the location of the installed cmake version and the libomp version may have to be changed to match the location when these were installed by homebrew.

Alternatively macports can be used in a very similar way to homebrew. Install libomp with \texttt{sudo port install libomp}. The location of omp.h may be different than expected by CMake, which may be fixed with \texttt{sudo ln -s /opt/local/include/libomp/omp.h /opt/local/include/omp.h}. or \texttt{sudo port install libomp +top\_level}.

\section{Parallelization}
NISE is equipped with support for MPI and OpenMP, to provide a tiered parallelization solution. Currently, both the time consuming 2DIR and 2DUVvis techniques support MPI. The linear techniques do not have a parallel implementation as they are generally fast. (Techniques relying on LAPACK including Luminescence may use the MLK OpenMP support for limited speedup.)

For the two-dimensional techniques, it is recommended to understand the implemented approach for parallelization in order to achieve good performance. Each run will calculate a specified number of samples, each for 21 different polarization directions. The calculation time for each polarization direction is determined by the chosen values for {\tt t1max}, {\tt t2max} and {\tt t3max}.

All polarization directions may efficiently be calculated in parallel using MPI, distributed over all registered tasks (more explanation follows later). As long as you have sufficiently many samples, this will scale very well. In general, it is recommended to have a fraction or multiple of 21 as number of tasks, in order to make sure that no cores are simply waiting around after completing their part of the calculations.

Within each polarization direction, loops over the t1 coherence times are parallelized using OpenMP. Due to communication overhead and data sharing difficulties, this does not scale as well as the MPI parallelization. If possible, it is recommended to overprovision your cores, i.e. to make the system spawn more threads than there are cores available. The larger the computation per polarization direction (so higher t2, t3, system size {\tt singles}), the better this part will scale. It is recommended that the number of OpenMP threads is either small compared to {\tt t1max} or that {\tt t1max+1} is equal to an integer times the number of OpenMP threads.

For example, to run 4 tasks with each 12 threads with OpenMPI, use the following command:\\
{\tt mpirun -np 4 -x OMP\_NUM\_THREADS=12 --bind-to-socket ~/pathToNISE/NISE inputFile}\\
For cluster computing refer to the manual for the cluster. Special commands as {\tt srun} may be required for {\tt SLURM} systems. The MPI implementation require all input files to be located at a disk available to all nodes.

\subsection{Efficiency considerations}
Some considerations and examples to achieve higher performance:
\begin{itemize}
\item As OpenMP parallelization uses shared memory, it is necessary to limit each task to one node. If possible, it is recommended to limit a task even to one socket, or in case of more modern chips, 1 NUMA node. However, this might make the runtime for one polarization direction too high, so it might be worthwhile to trade some efficiency for shorter runtimes.
\item It is recommended to overprovision your OpenMP threads by a factor of 2-3. So if a NUMA node has 12 cores, you could pin the threads of this task to this NUMA node and tell OpenMP to create 24-36 threads. Thread pinning differs per OS and more details can be found online. Many popular workload schedulers like SLURM, and some MPI implementations, offer this built-in (for example, {\tt --bind-to-socket} in the command above).
\item The most efficient workload division also depends on the problem size, for larger problems with fewer samples, the OpenMP scaling is more efficient than for smaller problems with more samples.
\item For example, on a machine with 2 12-core Xeon processors (single NUMA node per socket), it is most efficient to run 4 tasks, with 12 threads assigned to each task (overprovisioning). However, for very large problems with only 2 or 3 samples, it might be better to scale down to 2 tasks, each with 24 threads. For smaller problems with many samples, 8 tasks with 6 threads might be better. In general, it is good to do some quick performance tests beforehand.
\end{itemize}

\section{Changelog}
\subsection{Version 3.4 in progress}
{\small Work by Thomas Jansen and Kim van Adrichem}
\begin{itemize}
\item openMP parallel CD and DOS were implemented
\item Diffusion calculations were added
\item On the fly transition-dipole and extended-dipole coupling schemes were impemented
\item Added project file option to project on substructures
\item Added inhomogeneous and homogeneous apodization functions
\item Added automatic check on Singles keyword providing warning to user if it may be incorrect
\item Implemented speed-up (x2) of 2D calculations by removing if statements in propagation of double excited states (2DIR by Kim van Adrichem)
\end{itemize}
\subsection{Version 3.3}
{\small Work by Thomas Jansen}
\begin{itemize}
\item Extended MPI support to 2DIR
\item Implemented true two-state behaviour for the *UVvis techniques
\item Added linear dichroism
\item Changed timing information for two-dimensional calculations to percentage of full calculation based
\item Important change in naming of 2DIR sub techniques to contain IR at the end (GBIR, SEIR, EAIR, etc.).
\end{itemize}
	\subsection{Version 3.2}
{\small Work by Floris Westerman}
\begin{itemize}
\item Added CMake build system and improved cross-platform compatibility
\item Added MPI support to offer significantly better scaling across multiple nodes, instead of just a single one
\item Improved code efficiency, around 4x speed-up of main algorithm code of *UVvis techniques
\end{itemize}

\subsection{Version 3.1}
\begin{itemize}
\item Included OpenMP support for two-dimensional calculations
\end{itemize}

\include{input}

\include{tutorial}

\include{techniques}

\include{developers}

\chapter{Acknowledgement}
The following people are kindly acknowledged for their contribution through helpful discussions, ideas, and feedback:
Foppe de Haan, Arend Dijkstra, Alexander Paarmann, Carsten Olbrich, Jim Skinner and his group, Carlos Baiz, Chungwen Liang, Santanu Roy, Ana V. Cunha, Andy Sardjan, and Ariba Javad. 

The NWO is gratefully acknowledged
for financial support making is possible to write the initial versions of this code through VENI and VIDI grants. 

\bibliography{Merged2016}%Rochester-9.bib}
\end{document} 
